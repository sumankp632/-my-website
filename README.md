Real-Time Language Translation Using Neural Machine Translation (NMT)
ğŸš€ A multilingual translation model using the mT5 Transformer to break language barriers in real-time.



ğŸ“Œ Overview
This project implements a real-time translation system using the mT5-base model from Hugging Face. It can translate text between multiple languages, such as English (en), Japanese (ja), and Chinese (zh). The model is fine-tuned on a multilingual dataset and optimized for accurate translation.


ğŸš€ How It Works
1ï¸âƒ£ Tokenization
The input text is first tokenized using the mT5-base tokenizer and preprocessed.

python
Copy
Edit
input_ids = tokenizer.encode(
    '<zh> Hello, how are you?', return_tensors="pt", padding=True, truncation=True
)
2ï¸âƒ£ Model Inference
The encoded input is passed through the mT5 model to generate the translation.

python
Copy
Edit
output_tokens = model.generate(input_ids, num_beams=5)
3ï¸âƒ£ Decoding
The generated output tokens are converted back into human readable text.

python
Copy
Edit
translated_text = tokenizer.decode(output_tokens[0], skip_special_tokens=True)
print("Translation:", translated_text)
ğŸ“Š Evaluation & Results
The model was tested using BLEU and ROUGE scores for translation accuracy.
Example output:

css
Copy
Edit
Input: "What is your name?"
Output (Chinese): "ä½ çš„åå­—æ˜¯ä»€ä¹ˆ?"
ğŸ“Œ Features
âœ… Supports English, Japanese, and Chinese
âœ… Uses mT5 Transformer for high-quality translation
âœ… Implements Beam Search for better accuracy
âœ… GPU acceleration with CUDA support

ğŸ“Œ Future Enhancements
ğŸ”¹ Expand language support (e.g., Spanish, German)
ğŸ”¹ Deploy as an API or web app
ğŸ”¹ Improve translation fluency with fine-tuning

ğŸ“ Contributors
Vandhana â€“ Model Training & Evaluation,
Ridhin â€“ Data Preprocessing & Tokenization,
Shiva â€“ Model Inference & Testing,
Angel â€“ UI & Documentation
ğŸ“œ License
This project is licensed under the MIT License.
